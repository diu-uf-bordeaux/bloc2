{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Theorem\n",
    "\n",
    "## Motivation\n",
    "\n",
    "### Exemple revisité\n",
    "\n",
    "On a vu ce matin quelques exemples d'algoritmes récursif, dont la complexité peut être calculées en résolvant une récurrence. On peut aussi faire une analyse itératif de comportement de l'algorithme :\n",
    "\n",
    "Pour le tri par fusion, on peut regarder les appels récursifs niveau par niveau et compter le nombre d'opérations élémentaires à exécuter, puis, faire une somme.\n",
    "\n",
    "(à détailler).\n",
    "\n",
    "Cependant, analyser d'une manière itératif un algorithme récursif, c'est pas génial. Mieux vaudrait pouvoir analyser un algorithme récursif d'une manière récursif !\n",
    "\n",
    "Nous avons observé ce matin (ou on peut chosir de garder cette observation pour l'après-midi, kisè), que si on note $T(n)$ la complexité au pire cas de tri par fusion d'une liste de longueur $n$, alors on a \n",
    "\n",
    "$$\n",
    "T(n) \\le 2 \\cdot T(n/2) + O(n).\n",
    "$$\n",
    "\n",
    "Pourquoi pas l'égalité ? D'où vient l'inégalité ? On vous a menti ce matin en présentant une égalité de cette forme ?\n",
    "\n",
    "Il est probable que le pire cas pour trier une liste de longueur $n$ est tel qu'au moins une des sous-listes (la première moitié et la seconde moitié d'une liste donnée) est un pire cas pour trier une liste de longueur $n/2$, mais rien ne dit que les deux sont telles...\n",
    "\n",
    "Autrement dit, si $T(n/2)$ représente le nombre maximum d'opérations élémentaires à effectuer afin de trier une liste de longueur $n/2$, l'algorithme de tri par fusion garantit de savoir trier une liste de longueur $n$ en temps borné par $2\\cdot T(n/2) + O(n)$; cependant, rien ne justifie que ce temps d'exécution est vraiment réalisable. \n",
    "\n",
    "Le nombre $2\\cdot T(n/2) + O(n)$ représente donc une borne supérieure sur la complexité de tri d'une liste de longueur $n$. Même au pire des pires cas, on ne le dépasse jamais.\n",
    "\n",
    "De toute façon, le $O(n)$ cache aussi lui-même une borne supérieure et non pas forcément une valeur exacte de complexité de la fonction de fusion de deux listes triées..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résolution de récurrence : qu'est ce qui passe et qu'est ce qui ne passe pas ?\n",
    "\n",
    "Abstrayons de l'exemple de tri par fusion uniquement l'inégalité \n",
    "\n",
    "$$\n",
    "T(n) \\le 2 \\cdot T(n/2) + O(n)\n",
    "$$\n",
    "\n",
    "disant que pour résoudre le problème donné sur les données de taille $n$, il suffit de résoudre le même problème sur deux jeux d'entrée de taille $n/2$ chacun, et puis à partir des deux solutions partielles on peut retrouver la solution finale à retourner par un algorithme de complexité $O(n)$.\n",
    "\n",
    "(Bien évidemment, l'inégalité récurrence est initialisée quelque part, on peut y ajouter des conditions de départ disant que pour les données de taille constant (comme $n=0$ ou $1$), l'algorithme résout le problème en temps constant, sans faire de la récursivité.)\n",
    "\n",
    "Que peut-on dire de la complexité de notre algorithme ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercice:* Soit $A$ un algorithme dont la complexité satisfait $$\n",
    "T(n) \\le 2 \\cdot T(n/2) + c\\cdot n\n",
    "$$ avec $T(1)=c_1$.\n",
    "\n",
    "Peut-on en déduire que $T(n) \\in O(n^2) $ ?\n",
    "\n",
    "*Solution:* Essayons de trouver une constante $K$ (sans chercher à la minimiser) telle que $T(n) \\le K\\cdot n^2$:\n",
    "\n",
    "La condition de départ implique que $K \\ge c_1$.\n",
    "\n",
    "Ensuite, pour tout $n\\ge 2$, si $K$ existe, alors on a \n",
    "\n",
    "$$\n",
    "T(n) \\le 2 \\cdot T(n/2) + c\\cdot n \\le 2 \\cdot K \\cdot (n/2) ^2 + c \\cdot n = (K/2) \\cdot n^2 + c \\cdot n = \\left(\\frac12 + \\frac{c}{Kn}\\right) \\cdot K \\cdot n^2\n",
    "$$\n",
    "\n",
    "Afin de pouvoir conclure, il suffit de faire de sorte que $\\frac12 + \\frac{c}{Kn} \\le 1$, ce qui donne que $K$ devrait satisfaire $K \\ge \\frac{2c}{n}$ pour tout $n\\ge 2$.\n",
    "\n",
    "Il suffit donc poser $K=max(c,c_1)$ et tout marche bien. \n",
    "\n",
    "On peut même refaire la démonstration par récurrence avec cette valeur retrouvé de $K$, en faisant des 'pas artificiels':\n",
    "\n",
    "$$T(n) \\le 2 \\cdot T(n/2) + c \\cdot n \\le K/2 \\cdot n + c/2 \\cdot n^2 \\ le K \\cdot n^2$$\n",
    "\n",
    "où la première inégalité est une propriété de base de l'algorithme, la deuxième est vraie pour tout $n\\ge 2$, et la troisième est donnée par $c \\le K$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercice:* Soit $A$ un algorithme dont la complexité satisfait $$\n",
    "T(n) \\le 2 \\cdot T(n/2) + c\\cdot n\n",
    "$$ avec $T(1)=c_1$.\n",
    "\n",
    "Peut-on en déduire que $T(n) \\in O(n) $ ?\n",
    "\n",
    "*Solution:* Essayons de trouver une constante $K$ (sans chercher à la minimiser) telle que $T(n) \\le K\\cdot n$:\n",
    "\n",
    "La condition de départ implique que $K \\ge c_1$.\n",
    "\n",
    "Ensuite, pour tout $n\\ge 2$, si $K$ existe, alors on a \n",
    "\n",
    "$$\n",
    "T(n) \\le 2 \\cdot T(n/2) + c\\cdot n \\le 2 \\cdot K \\cdot (n/2) + c \\cdot n = (K+c) \\cdot n \n",
    "$$\n",
    "\n",
    "ce qui est loin de ce qu'on espère de garantir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observons l'exemple précédent de plus près. Quand on passe de $n/2$ à $n$, le facteur avec lequel la taille de l'entrée est multipliée, passe de $K$ à $K+c$. Ce que l'on peut espérer le mieux de trouver comme complexité est $n$ fois un facteur qui a une croissance telle qu'en doublant l'argument, il est augmenté d'une constante. C'est la définition presque du logarithme !\n",
    "\n",
    "Nous avons vérifié ce matin que $T(n) = K\\cdot n\\log n$ est une solution de la récurrence $$\n",
    "T(n) \\le 2 \\cdot T(n/2) + c\\cdot n\n",
    "$$ avec une égalité.\n",
    "\n",
    "Cela veut dire que $n\\log n$ est une fonction de l'ordre de croissance le plus petit possible parmi toutes les fonctions vérifiant la récurrence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le théorème : formulation \n",
    "\n",
    "### Avant propos\n",
    "\n",
    "L'exemple précédent peut se généraliser de la manière suivante.\n",
    "\n",
    "Soit $A$ un algorithme récursif qui fonctionne de la forme spéciale suivante : Afin de trouver une solution pour une entrée de taille $n$, il la transforme en plusieurs entrées de taille strictement inférieure à $n$, pour lesquelles il s'appelle lui-même. Les solutions dites partielles obtenues ainsi sont combinées pour obtenir une solution (complète) à retourner. (Important à dire que pour des valeurs petites de $n$ (jusqu'à une constante), la solution est calculée sans faire de la récursivité.)\n",
    "\n",
    "Que peut-on dire de la complexité de cet algorithme ? (Énoncé si vaguement, rien ;) )\n",
    "\n",
    "La version la plus courrante est la suivante : Afin de trouver une solution pour une entrée de taille $n$, l'algorithme la transforme en $a$ entrées de taille $n/b$, pour lesquelles il s'appelle lui-même. Les solutions partielles sont combinées pour obtenir une solution complète à retourner.\n",
    "\n",
    "### Quelques intuitions \n",
    "\n",
    "Bien évidemment, plus grand la valeur de $a$, plus complexe l'algoritme.\n",
    "\n",
    "Pareil, plus grand la valeur de $b$, moins complexe l'algorithme.\n",
    "\n",
    "Peut-être que découper l'entrée de taille $n$ en trois partie de taille $n/3$ ou deux partie de taille $n/2$ devrait donner la même complexité. Ou pas ?\n",
    "\n",
    "À part du branchement, qui est défini par les valeurs de $a$ et de $b$, il y a aussi le calcul qui se fait à chaque étape : la réduction (transformer une entrée de taille $n$ en plusieurs plus petites), et la recombinaison (pour le tri fusion, c'est la fusion).\n",
    "\n",
    "La question principale ici est une question de dichotomie : Qui est-ce qui décide réellement de la complexité de l'algorithme ? Qu'est-ce qui coûte plus cher : Faire toutes les branchements ou calculer toutes les recombinaisons ?\n",
    "\n",
    "### Le théorème\n",
    "\n",
    "Considérons un algorithme qui opère comme suit : une instance de taille $n$ est partagée en $a$ sous-problèmes, chacun de taille $n/b$. On résout les problèmes partiels puis on recombine les solutions partielles en une solutions du problème initial. On note $f(n)$ la complexité de la procédure de partager une instance en sous-problèmes et de la procédure de recombiner les solutions initiales en une solution complète.\n",
    "\n",
    "Autrement dit, la compexité de l'algorithme vérifie\n",
    "\n",
    "$$\n",
    "T(n)=a\\cdot T(n/b) + f(n)\n",
    "$$\n",
    "\n",
    "avec $a \\ge 1$ et $b > 1$.\n",
    "\n",
    "Alors, on peut déterminer la compexité $T(n)$ en fonction de $a$, de $b$ et de l'ordre de grandeur de $f(n)$ de la manière suivante :\n",
    "\n",
    "Posons $c=\\log_b a$. \n",
    "\n",
    "Cas 1: Si $f(n) = O(n ^{c'})$ avec $c'< c$, alors $T(n)=\\Theta(n^c)$.\n",
    "\n",
    "Cas 2: (je vais préciser en dernier, pour garder de la suspense)\n",
    "\n",
    "Cas 3: Si $f(n) = \\Omega(n^{c'})$ avec $c'>c$ (plus une condition de régularité que je vais spécifier plus tard), alors $T(n)=\\Theta(f(n))$.\n",
    "\n",
    "### Encore de l'intuition\n",
    "\n",
    "Cas 1 parle du cas où le coût de recombiner est négligeable par rapport au coût de branchement. La constante $c$ décrit le taux de croissance de branchement -- la richesse de l'arbre des appels récursifs. Même sans recombiner à faire, si on remplace la fonction $f(n)$ par une constante, le théorème dit qu'on ne peut pas faire mieux que $n^c$ -- c'est le coût de l'arbre des appels récursifs.\n",
    "\n",
    "Cas 3 parle du cas où le coût de la dernière recombinaison surpasse le coût de tous les branchements antérieurs. Si l'arbre est 'peu dense', mais à chaque noeud il faut faire une manipulation coûteuse, l'élément décisif pour la complexité est la toute dernière manipulation sur le noeud racine.\n",
    "\n",
    "### Complétons la formulation\n",
    "\n",
    "Cas 3: ... et qu'il existe une constante $k<1$ telle que, pour $n$ assez grand, on a $kf(n)\\ge a f(n/2)$, alors...\n",
    "\n",
    "Cas 2: Si $f(n)=\\Theta(n^c \\cdot \\log^k n)$ avec une constante $k\\ge 0$ (ce qui couvre aussi le cas $f(n)=\\Theta(n^c)$), alors $T(n)=\\Theta (n^{c+1} \\cdot \\log^k n)$.\n",
    "\n",
    "# Preuve\n",
    "\n",
    "Comme on a fait pour le tri par fusion, il suffit de déviner la fonction qui vérifie la récurrence pile poile, et vérifier qu'elle la vérifie vraiment. Pas de secret donc pour la démonstration du théorème, c'est de la manipulation formelle qu'on va pas détailler. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercice:* Appliquer le théorème maître pour déterminer la complexité du tri par fusion de nouveau. \n",
    "\n",
    "*Exercice:* Pareil pour l'algorithme pour trouver l'élément majoritaire en cherchant dans deux sous-listes de taille $n/2$.\n",
    "\n",
    "*Exercice:* (de Paul Dorbec; exercice inspiré de 'Algorithms', de S. Dasgupta, C. Papadimitriou, U. Vazirani)\n",
    "\n",
    "Imaginez que vous ayez à choisir entre les trois algorithmes suivants pour résoudre un problème de taille $n$:\n",
    "\n",
    "- L'algorithme A résout le problème en le divisant en 5 sous problèmes de taille $\\frac{n}{2}$, en les résolvant récursivement, puis en recombinant les solutions en temps linéaire.\n",
    "- L'algorithme B résout le problème en résolvant récursivement deux problèmes de taille $n-1$ puis en combinant les solutions en temps constant.\n",
    "- L'algorithme C résout le problème en le découpant en 9 sous-problèmes de taille $\\frac{n}{3}$ qui sont résolus récursivement, puis en combinant les solutions avec un algorithme quadratique (en $O(n^2)$).\n",
    "\n",
    "Pour chaque algorithme, vous donnerez explicitement la formule de récurrence satisfaite par la complexité de l'algorithme, et l'ordre de grandeur du temps d'exécution.\n",
    "Quel algorithme a la meilleure complexité asymptotique?\n",
    "\n",
    "\n",
    "*Correction:* On applique deux fois le master Theorem (pour A et C), et on fait explicitement la récurrence pour B.\n",
    "\n",
    "Pour l'algorithme A, la formule de récurrence est $T(n) = 5 T(n/2) +O(n)$. On obtient les constantes $a=5$, $b=2$, et $f(n)=n^1$. On est dans le premier cas du théorème, on obtient une complexité en $O(n^{\\log_2(5)})\\approx n^{2.32}$.\n",
    "\n",
    "Pour l'algorithme B, le théorème ne s'applique pas. On a $T(n)=2 T(n-1)+c$ où $c$ est une constante ( i.e. $O(1)$). En définissant $T'(n)=T(n)-c$, on a $T'(n) = 2T'(n-1)$. Donc le résultat est en $O(2^n)$, cet algo est pourri!\n",
    "\n",
    "Pour l'algorithme $C$, la formule de récurrence est $T(n) = 9 T(n/3) +O(n^2)$. Le master theorem s'applique avec $a=9$, $b=3$, $f(n)=n^2$. On est dans le cas $c=2=\\log_3(9)$ et donc la complexité est en $O(n^2 \\log(n))$.\n",
    "\\end{itemize}\n",
    "Le meilleur choix est ici l'algo $C$.\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Exercice:* Exercice 7 de Lille"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pour aller plus loin\n",
    "\n",
    "Il n'est pas toujours possible de découper un problème en plusieurs sous-problèmes de la même taille. \n",
    "\n",
    "Pour une récurrence de forme\n",
    "\n",
    "$$T(n)=\\sum_{i=1}^m T(\\alpha_i n) + f(n)$$\n",
    "\n",
    "avec des constantes $0<\\alpha_i<1$ ($m\\ge 1$), si $f(n)=\\Theta(n^c)$ pour un $c>0$, alors, \n",
    "\n",
    "Cas 3: Si $\\sum_{i=1}^m \\alpha_i^c <1$, alors $T(n)=\\Theta(n^c)$.\n",
    "\n",
    "Cas 2: Si $\\sum_{i=1}^m \\alpha_i^c =1$, alors $T(n)=\\Theta(n^c\\log n)$.\n",
    "\n",
    "Cas 1: Si $\\sum_{i=1}^m \\alpha_i^c >1$, alors $T(n)=\\Theta(n^\\gamma)$ avec $\\gamma$ défini par $\\sum_{i=1}^m \\alpha_i^\\gamma =1$.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
